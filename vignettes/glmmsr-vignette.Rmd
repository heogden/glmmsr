---
title: "Fitting GLMMs with `glmmsr`"
author: "Helen Ogden"
date: " "
output: rmarkdown::html_vignette
bibliography: glmmsr.bib
vignette: >
  %\VignetteIndexEntry{glmmsr}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  
---

# Introduction

Generalized linear mixed models (GLMMs) are an important and widely-used model 
class. In R, it is possible to these models with the 
`lme4` package [@lme4], but there are some
limitations. First, except in very simple cases, `lme4` uses a Laplace 
approximation to the likelihood for inference, which may be of poor quality in 
some cases. Second, it is difficult to fit some GLMMs, such as pairwise 
comparison models, with `lme4`. 
The `glmmsr` package aims to offer progress on both of these problems. 

A user must choose which method to use to approximate
the likelihood. In addition
to the Laplace and adaptive Gaussian quadrature approximations,
which are borrowed from `lme4`, the likelihood may
be approximated by the sequential reduction approximation [@Ogden2015], which
gives an accurate approximation to the likelihood in some situations
where it is not possible to use adaptive Gaussian quadrature.

The interface of `glmmsr` allows easy fitting of pairwise comparison 
and many other interesting 
models, which are difficult to fit with `lme4`.

# Approximations to the likelihood

##  Example: a two-level model
Suppose that our data are clustered, so that we observe $y_{ij}$
for $i=1, \ldots, m$ and $j = 1, \ldots, m_i$,
where $y_{ij}$ is the $j$th obervation from the
$i$th cluster.
We model $Y_{ij} \sim \text{Bernoulli}(p_{ij})$
where $\log(p_{ij} / 1 - p_{ij}) = \alpha + \beta x_{ij} + \sigma u_i,$
and $b_i \sim N(0, \sigma^2)$.
The data `two_level`, available in
`glmmsr`, is simulated from a binary two-level model
with $m_i = 2$ observations per cluster, and 
with $m = 50$ clusters in total.  

We want to infer the parameters
$\theta = (\alpha, \beta, \sigma)$ of the model. 
The likelihood may be written as
$$L(\theta) = \int g(u; \theta) du,$$
where $$g(u; \theta) = \prod_{i=1}^n \prod_{j=1}^{m_i} Pr(Y_{ij} = y_{ij} | u_i, \theta) \phi(u_i).$$

For this model, and for other GLMMs, 
the likelihood function is an integral over the latent variables.
It is not possible to compute this integral exactly, except in the
case of a linear mixed model.
In \code{glmmsr}, the interface for fitting this model is
```{r, eval = FALSE}
glmm(response ~ covariate + (1 | cluster), data = two_level,
     family = binomial, method = method)
```
where the choice of `method` determines the approximation
to the likelihood. There is no default for `method`. This is a deliberate
choice, to highlight to the user that some approximation must be used, and
that the choice of this approximation might affect the resulting inference.

### The Laplace approximation (`method = "Laplace"`)
The Laplace approximation works by approximating the integrand
by a function proportional to a Gaussian density.

```{r}
library(glmmsr)
(mod_Laplace <- glmm(response ~ covariate + (1 | cluster), data = two_level,
                     family = binomial, method = "Laplace"))
```
We have obtained some parameter estimates, but do not know if these
are close to the maximum likelihood estimates. Typically the structure
of the data gives some hints about the quality of the Laplace approximation:
if the data are `sparse`, in that there is only a small number
of observations per random effect, the Laplace approximation may be a poor
approximation to the likelihood. This is certainly the case here, since
we only have two observations per cluster, and we would like to try
using a more accurate approximation to the likelihood.

### Adaptive Gaussian quadrature (`method = "AGQ"`)
For a two-level model, where each observation is contained within
a single cluster, the likelihood simplifies into a product of
one-dimensional integrals. 
$$L(\theta) = \prod_{i=1}^m  \prod_{j=1}^{m_i} Pr(Y_{ij} = y_{ij} | u_i, \theta) \phi(u_i) \int  du_i.$$
In that case, we can use adaptive Gaussian
quadrature with `nAGQ` points to approximate each of the one-dimensional 
integrals.
```{r}
glmm(response ~ covariate + (1 | cluster), data = two_level,
     family = binomial, method = "AGQ", control = list(nAGQ = 16),
     prev_fit = mod_Laplace)
```
The estimate of the random effect standard deviation is significantly
larger than that obtained using the Laplace approximation to the likelihood.

Unfortunately, we can only use this method for a simple two-level model.
Next, we consider a situation where we cannot use this method.

## Example: three-level model

Now suppose that each of the clusters is itself
contained within a larger group, so that we observe $y_{ijk}$
for $i=1, \ldots, m,$ $j = 1, \ldots, m_i$ and $k = 1, \ldots, m_{ij}$,
where $y_{ijk}$ represents the $k$th observation in the $j$th cluster
of the $i$th group.
We have $Y_{ijk} \sim \text{Bernoulli}(p_{ijk})$
where $\log(p_{ijk} / 1 - p_{ijk}) = \alpha + \beta x_{ijk} + b_i + c_j,$
$b_i \sim N(0, \sigma_b^2)$ and $c_j \sim N(0, \sigma_c^2)$.

The data `three_level`, available in
`glmmsr`, is simulated from a binary two-level model
with $m_{ij} = 2$ observations per cluster, $m_i = 2$ clusters
per group, and $m = 50$ clusters in total.  

We may fit the model with the Laplace approximation to the likelihood
```{r}
(mod_3_Laplace <- glmm(response ~ covariate + (1 | cluster) + (1 | group),
                       data = three_level, family = binomial, method = "Laplace"))
```
The structure is sparse -- there is little
information provided by the data about the value of each random effect --
so we might again question whether the Laplace approximation is of
sufficiently high quality.

We could try to increase the accuracy of the approximation by using 
adaptive Gaussian quadrature to approximation the integral.
```{r, error = TRUE}
glmm(response ~ covariate + (1 | cluster) + (1 | group), data = three_level,
     family = binomial, method = "AGQ", control = list(nAGQ = 16))
```
We get an error message, because the likelihood does not reduce to a product
of one-dimensional integrals in this case, as it does for a two-level model.

### The sequential reduction approximation (`method = "SR"`)
The sequential reduction approximation to the likelihood is described in
[@Ogden2015]. 

The approximation is controlled by a parameter `nSL`, the 
level of sparse grid storage. `nSL` is a non-negative integer, 
where `nSL = 0` corresponds to the Laplace approximation, 
and increasing `nSL` gives a more accurate approximation to the likelihood.
In the special case of a two-level model, sequential reduction is equivalent
to adaptive Gaussian quadrature with $2^{n_{SL}}$.
In typical examples, `nSL = 3` is large enough to
give an accurate likelihood approximation.

In the three-level example,
can use `glmm` to fit the model using the sequential reduction
approximation, with 3 sparse grid levels.
```{r}
(mod_3_SR <- glmm(response ~ covariate + (1 | cluster) + (1 | group),
                data = three_level, family = binomial, method = "SR",
                control = list(nSL = 3), prev_fit = mod_3_Laplace))
```
The estimates of the random effects standard deviations
are larger than the corresponding estimates from the Laplace approximation.

To check the quality of the approximation, we could increase the level
of sparse grid storage
```{r}
(mod_3_SR_4 <- glmm(response ~ covariate + (1 | cluster) + (1 | group),
                data = three_level, family = binomial, method = "SR",
                control = list(nSL = 4), prev_fit = mod_3_SR))
```
We see that the parameter estimates are very close to those obtained with
the lower level of sparse grid storage, and conclude that we are likely
to have estimates close to the exact maximum likelihood estimates.

# The subformula interface
The interface of `glmm` also allows fitting
of some more complex models which are not easy to fit using `lme4`.

A typical call using this extended interface looks like
`glmm(formula, subformula, data, family, method, control)`
where

1. `formula` may contain one or more terms surrounded with `Sub(.)`. We call the
expression contained within `Sub(.)` a **substitution expression**. This is a 
mathematical expression dictating how the response depends on a
**substituted variable**: a dummy variable not contained in `data`. 

2. `subformula` contains a **subformula** for each substituted variable, which
describes how the substituted variable depends on 
covariates.

Next, we consider an example of the type of model we might want to fit using 
this interface.

### Pairwise competition models
Suppose that we observe the outcome of a set of matches played between pairs of 
players, and that we also observe some covariates $x_{i}$ for each player $i$. 
We suppose that each player $i$ has an 'ability' $\lambda_i$, and that
\[Pr(\text{$i$ beats $j$} | \lambda_i, \lambda_j) = g(\lambda_i - \lambda_j),\]
where $g(.)$ is an inverse link function. If we are interested in how the 
ability depends on the covariates, we might model
\[\lambda_i = \beta x_{i} + b_i, \]
where $b_i \sim N(0, \sigma^2)$, and $\beta$ and $\sigma$ are unknown
parameters.

This is a structured pairwise competition model. The `BradleyTerry2` package 
[@BradleyTerry2]
provides a good interface to fit these models, but it uses Penalized Quasi 
Likelihood (PQL) for inference if there are random effects in the model. PQL
is often a poor approximation to the true likelihood. 

We wrote down the structured pairwise competition model using a two-step 
approach: first we described how the response depends on the unknown 
abilities, then we wrote down how the abilities depend on covariates. This type 
of model can be written quite naturally using the subformula interface. We have 
a formula
`response ~ 0 + Sub(ability[player1] - ability[player2])`, 
where `ability` is a substitution variable. We write
down a corresponding subformula
`ability[i] ~ 0 + x[i] + (1 | i)`.
Here `player1` and `player2` are factors with common levels, and `x`
is a vector of player-specific covariates, where the ordering of the players is
given by the levels of `player1` and `player2`. The index `i` is arbitrary, and 
is deduced automatically from the levels of `player1` and `player2`.

## Example: `chameleons` data

@Stuart-Fox2006 study contests between male Cape dwarf chameleons.
The data are available in `BradleyTerry2`: see `?chameleons` for
more details. The model suggested by 
@Stuart-Fox2006 includes an experience effect: we allow
the ability for each chameleon to change as the tournament progresses.

In particular, for each chameleon $i$ at each match $m$, we count the 
number of wins for that chameleon in its (at most) two previous matches. 
We start by constructing a matrix `prevwins2` to record these counts.
```{r}
library(BradleyTerry2)
winner <- chameleons$winner$ID
loser <- chameleons$loser$ID
match <- 1:length(winner)

wins <- matrix(0, nrow = length(levels(winner)), ncol = length(winner))
wins[cbind(as.numeric(winner), match)] <- 1

losses <- matrix(0, nrow = length(levels(winner)), ncol = length(winner))
losses[cbind(as.numeric(loser), match)] <- 1

contests <- wins + losses

find_prev2 <- function(wins_im, contests_im) {
  m <- min(2, sum(contests_im))
  if(m > 0) {
    res <- sum(wins_im[rev(which(contests_im > 0L))[1:m]])
  } else{
    res <- 0
  }
  res
}

prevwins2 <- matrix(0, nrow = nrow(wins), ncol = ncol(wins))
for(i in 1:nrow(wins)) {
  for(m in 2:ncol(wins)) {
    prevwins2[i, m] <- find_prev2(wins[i, 1:(m-1)], contests[i, 1:(m-1)])
  }
}
```

Then we fit the model
```{r, eval = FALSE}
library(glmmsr)
resp <- rep(1, length(winner))
cham_dat <- c(list(resp = resp, winner = winner, loser = loser, 
                  match = match, prevwins2 = prevwins2), 
             as.list(chameleons$predictors))

cham_mod <- glmm(resp ~ 0 + Sub(ability[winner, match] -
                                ability[loser, match]),
                ability[i, m] ~ 0 + prevwins2[i, m] + ch.res[i] 
                                    + prop.main[i] + (1 | i),
                family = binomial, data = cham_dat)

print(summary(cham_mod), correlation = FALSE, show.resids = FALSE)
```
We can fit the same model with `BradleyTerry2`.

```{r}
cham_mod_BTm <- BTm(player1 = winner, player2 = loser,
                    formula = ~ prev.wins.2 + ch.res[ID] + prop.main[ID] + (1|ID),
                    id = "ID", data = chameleons)
summary(cham_mod_BTm)
```
In this case the two fits are the same, because in both
cases the random effects variance is estimated to be zero.

## References
